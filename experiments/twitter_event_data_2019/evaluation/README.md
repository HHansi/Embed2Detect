## Evaluation
Evaluating methods to compare detected events with ground truth

### Available metrics
- F-Score (F1)
- Precision
- Recall
- Micro Keyword Recall

### Running the evaluation
run [evaluate.py](https://github.com/HHansi/Embed2Detect/blob/master/experiments/twitter_event_data_2019/evaluation/evaluate.py) 
given the parameters;
- groundtruth_folder - path to ground truth folder
- file_name - name of the results folder which is formatted according to the <em>Results/Input format</em>

### Results/input format
The Output format of Embed2Detect which is further described as follows;
- folder of .txt files which represents the identified event windows
- each .txt file should be named by the corresponding time window following the same format used by ground truth (%Y_%m_%d_%H_%M)
- event words need to be saved as single word per line

### Notes
- This evaluation flow can be used to evaluate the results generated by any event detection technique using 
[Twitter Event Data 2019](https://github.com/HHansi/Twitter-Event-Data-2019). <br>
- It is recommended to apply same data cleaning methods used by event detection method for ground truth data also, in 
order to conduct more effective comparisons during the evaluation. <br>
To customise the data cleaning flow, change/switch the method; <em>preprocessing_flow</em> in 
[data_preprocessor.py](https://github.com/HHansi/Embed2Detect/blob/master/data_analysis/data_preprocessor.py) accordingly.